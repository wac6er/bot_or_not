{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/willcarrington/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "//anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "//anaconda3/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "//anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "//anaconda3/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "//anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "//anaconda3/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "//anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "//anaconda3/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "//anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "//anaconda3/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import time\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "#____________________________________________________________________________________________________________________\n",
    "#LOADING DATA + ORGANIZING DATASET (creating 'master_tweet' df for convenience, not used)\n",
    "#load data downloded from the 'bot repository'\n",
    "df_tfp = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\TFP.csv\\tweets.csv\")\n",
    "df_fsf = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\FSF.csv\\tweets.csv\")\n",
    "df_twt = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\TWT.csv\\tweets.csv\")\n",
    "df_int = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\INT.csv\\tweets.csv\")\n",
    "df_e13 = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\E13.csv\\tweets.csv\")\n",
    "\n",
    "\n",
    "df_tfp_u = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\TFP.csv\\users.csv\")\n",
    "df_fsf_u = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\FSF.csv\\users.csv\")\n",
    "df_twt_u = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\TWT.csv\\users.csv\")\n",
    "df_int_u = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\INT.csv\\users.csv\")\n",
    "df_e13_u = pd.read_csv(r\"C:\\Users\\wac6er\\Downloads\\Fake_project_dataset_csv.tar\\Fake_project_dataset_csv\\E13.csv\\users.csv\")\n",
    "\n",
    "#aggregating user tweet + bio text data\n",
    "pieces = (df_tfp,df_fsf,df_twt,df_int,df_e13)\n",
    "df_master_tweet = pd.concat(pieces, ignore_index = True)\n",
    "df_master_tweet['check'] = df_master_tweet['user_id']\n",
    "df_master_tweet['new'] = df_master_tweet['text'].str.split().str.get(0)\n",
    "df_master_tweet = df_master_tweet[df_master_tweet.new != 'RT']\n",
    "\n",
    "pieces = (df_tfp_u,df_fsf_u,df_twt_u,df_int_u,df_e13_u)\n",
    "df_master_user = pd.concat(pieces, ignore_index = True)\n",
    "df_master_user['check'] = df_master_user['user_id']\n",
    "df_master_user['new'] = df_master_user['text'].str.split().str.get(0)\n",
    "df_master_user = df_master_user[df_master_user.new != 'RT']\n",
    "\n",
    "#merging user tweets + bios\n",
    "df_all = pd.merge(df_master_tweet,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all = df_all.drop_duplicates()\n",
    "\n",
    "#printing length of the 'true user' dataset, then sampling that number from the different bot dataset tweets\n",
    "print(len(df_all[df_all['dataset']=='TFP']))\n",
    "\n",
    "number_real =89\n",
    "number_fake = 25 #roughly 1/4\n",
    "\n",
    "tfp_df = df_all[df_all['dataset']=='TFP'].sample(n=number_real)\n",
    "int_df = df_all[df_all['dataset']=='INT'].sample(n=number_fake)\n",
    "e13_df = df_all[df_all['dataset']=='E13'].sample(n=number_fake)\n",
    "fsf_df = df_all[df_all['dataset']=='FSF'].sample(n=number_fake)\n",
    "twt_df = df_all[df_all['dataset']=='TWT'].sample(n=number_fake)\n",
    "pcs = (tfp_df,int_df,e13_df,fsf_df,twt_df)\n",
    "\n",
    "#combining sampled dataset\n",
    "df_all = pd.concat(pcs, ignore_index = True)\n",
    "\n",
    "#____________________________________________________________________________________________________________________\n",
    "#CREATING SCATTER MATRIX + CLEANING\n",
    "min_chars = 3\n",
    "max_feats = 70000\n",
    "min_freq = 15\n",
    "\n",
    "#removing all non-alphanumeric numbers\n",
    "df_all['tweet_re'] = df_all.text.str.replace('[^a-zA-Z]', ' ')\n",
    "df_all['tweet_re'] = df_all['text'].map(lambda x:x.lower())\n",
    "\n",
    "#replacing meaningless characters\n",
    "df_all = df_all.replace({'object':' '}, regex = True)\n",
    "df_all = df_all.replace({'int':' '}, regex = True)\n",
    "df_all = df_all.replace({'http':' '}, regex = True)\n",
    "df_all = df_all.replace({'dtype':' '}, regex = True)\n",
    "\n",
    "#replacing all stopwords\n",
    "df_all['tweet_no_stop'] = df_all['tweet_re'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))\n",
    "\n",
    "#tokenizing results\n",
    "df_all['tweet_token'] = df_all['tweet_no_stop'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "#lemmatizing words to extract roots (distinguishing fished/fishing/fish for ex)\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(df_all['tweet_token'], allowed_postags=['NOUN', 'VERB'])\n",
    "\n",
    "#creating document weight vector\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=min_freq,\n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  \n",
    "                             max_features=max_feats)\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "\n",
    "c#RUNNING THE LDA MODEL, attempting 8 different topic numbers\n",
    "xxx=[2,3,4,5,6,7,8,9]\n",
    "chart = {'perp':[],'score':[]}\n",
    "for i in xxx:\n",
    "    lda_model = LatentDirichletAllocation(n_components=i,         \n",
    "                      max_iter=20,               \n",
    "                      learning_method='online',   \n",
    "                      random_state=100,          \n",
    "                      batch_size=128,            \n",
    "                      evaluate_every = -1,       \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "    chart['score'].append(lda_model.score(data_vectorized))\n",
    "    chart['perp'].append(lda_model.perplexity(data_vectorized))\n",
    "\n",
    "#chart contains accuracy variables\n",
    "chart = pd.DataFrame(chart)\n",
    "\n",
    "#checking out topics + their word weight\n",
    "def selected_topics(model, vectorizer, top_n=20):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]) \n",
    "\n",
    "selected_topics(lda_model, vectorizer)\n",
    "\n",
    "\n",
    "#loading LDA outputs (per tweet, % assignment to each topic) to a csv to merge with numeric tweet data such as likes/comments/followers/etc\n",
    "lda_model = LatentDirichletAllocation(n_components=2,         \n",
    "                  max_iter=20,               \n",
    "                  learning_method='online',   \n",
    "                  random_state=100,          \n",
    "                  batch_size=128,            \n",
    "                  evaluate_every = -1,       \n",
    "                  n_jobs = -1)\n",
    "\n",
    "lda_output2 = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=3,         \n",
    "                  max_iter=20,               \n",
    "                  learning_method='online',   \n",
    "                  random_state=100,          \n",
    "                  batch_size=128,            \n",
    "                  evaluate_every = -1,       \n",
    "                  n_jobs = -1)\n",
    "\n",
    "lda_output3 = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=4,         \n",
    "                  max_iter=20,               \n",
    "                  learning_method='online',   \n",
    "                  random_state=100,          \n",
    "                  batch_size=128,            \n",
    "                  evaluate_every = -1,       \n",
    "                  n_jobs = -1)\n",
    "\n",
    "lda_output4 = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5,         \n",
    "                  max_iter=20,               \n",
    "                  learning_method='online',   \n",
    "                  random_state=100,          \n",
    "                  batch_size=128,            \n",
    "                  evaluate_every = -1,       \n",
    "                  n_jobs = -1)\n",
    "\n",
    "lda_output5 = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=6,         \n",
    "                  max_iter=20,               \n",
    "                  learning_method='online',   \n",
    "                  random_state=100,          \n",
    "                  batch_size=128,            \n",
    "                  evaluate_every = -1,       \n",
    "                  n_jobs = -1)\n",
    "\n",
    "lda_output6 = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=7,         \n",
    "                  max_iter=20,               \n",
    "                  learning_method='online',   \n",
    "                  random_state=100,          \n",
    "                  batch_size=128,            \n",
    "                  evaluate_every = -1,       \n",
    "                  n_jobs = -1)\n",
    "\n",
    "lda_output7 = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=8,         \n",
    "                  max_iter=20,               \n",
    "                  learning_method='online',   \n",
    "                  random_state=100,          \n",
    "                  batch_size=128,            \n",
    "                  evaluate_every = -1,       \n",
    "                  n_jobs = -1)\n",
    "\n",
    "lda_output8 = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "df2 = pd.DataFrame(lda_output2)\n",
    "df3 = pd.DataFrame(lda_output3)\n",
    "df4 = pd.DataFrame(lda_output4)\n",
    "df5 = pd.DataFrame(lda_output5)\n",
    "df6 = pd.DataFrame(lda_output6)\n",
    "df7 = pd.DataFrame(lda_output7)\n",
    "df8 = pd.DataFrame(lda_output8)\n",
    "\n",
    "#saving\n",
    "df2.to_csv('lda2.csv')\n",
    "df3.to_csv('lda3.csv')\n",
    "df4.to_csv('lda4.csv')\n",
    "df5.to_csv('lda5.csv')\n",
    "df6.to_csv('lda6.csv')\n",
    "df7.to_csv('lda7.csv')\n",
    "df8.to_csv('lda8.csv')\n",
    "\n",
    "#____________________________________________________________________________________________________________________\n",
    "#USING LDA RESULTS + INTITIAL DATASET WITH NUMERIC ATTRIBUTES TO RUN A RANDOM FOREST\n",
    "#reading in data with LDA results (MANNUALLY MERGED DATASETS GIVEN LACK OF COMMON IDENTIFIER TO MERGE ON) \n",
    "lda2 = pd.read_csv('/Users/willcarrington/Downloads/lda2_final.csv')\n",
    "lda3 = pd.read_csv('/Users/willcarrington/Downloads/lda3_final.csv')\n",
    "lda4 = pd.read_csv('/Users/willcarrington/Downloads/lda4_final.csv')\n",
    "lda5 = pd.read_csv('/Users/willcarrington/Downloads/lda5_final.csv')\n",
    "lda6 = pd.read_csv('/Users/willcarrington/Downloads/lda6_final.csv')\n",
    "lda7 = pd.read_csv('/Users/willcarrington/Downloads/lda7_final.csv')\n",
    "lda8 = pd.read_csv('/Users/willcarrington/Downloads/lda8_final.csv')\n",
    "\n",
    "#creating dummy variables \n",
    "df_new = lda2\n",
    "df_new[['a','b','c','label','e']] = pd.get_dummies(lda5['dataset'])\n",
    "df_new['dataset']= label_encoder.fit_transform(df_new['dataset']) \n",
    "\n",
    "\n",
    "#running a random forest with + without LDA results to check for improvement\n",
    "xx = df_new[['0','1','statuses_count','followers_count','friends_count','listed_count']]\n",
    "xx=xx.fillna(0)\n",
    "\n",
    "y = df_new['label']\n",
    "x = xx\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "clf=RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_new = lda3\n",
    "df_new[['a','b','c','label','e']] = pd.get_dummies(lda5['dataset'])\n",
    "df_new['dataset']= label_encoder.fit_transform(df_new['dataset']) \n",
    "\n",
    "\n",
    "xx = df_new[['0','1','2','statuses_count','followers_count','friends_count','listed_count']]\n",
    "xx=xx.fillna(0)\n",
    "\n",
    "y = df_new['label']\n",
    "x = xx\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "clf=RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_new = lda4\n",
    "df_new[['a','b','c','label','e']] = pd.get_dummies(lda5['dataset'])\n",
    "df_new['dataset']= label_encoder.fit_transform(df_new['dataset']) \n",
    "\n",
    "\n",
    "xx = df_new[['0','1','2','3','statuses_count','followers_count','friends_count','listed_count']]\n",
    "xx=xx.fillna(0)\n",
    "\n",
    "y = df_new['label']\n",
    "x = xx\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "clf=RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_new = lda5\n",
    "df_new[['a','b','c','label','e']] = pd.get_dummies(lda5['dataset'])\n",
    "df_new['dataset']= label_encoder.fit_transform(df_new['dataset']) \n",
    "\n",
    "\n",
    "xx = df_new[['0','1','2','3','4','statuses_count','followers_count','friends_count','listed_count']]\n",
    "xx=xx.fillna(0)\n",
    "\n",
    "y = df_new['label']\n",
    "x = xx\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "clf=RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_new = lda6\n",
    "df_new[['a','b','c','label','e']] = pd.get_dummies(lda5['dataset'])\n",
    "df_new['dataset']= label_encoder.fit_transform(df_new['dataset']) \n",
    "\n",
    "\n",
    "xx = df_new[['0','1','2','3','4','5','statuses_count','followers_count','friends_count','listed_count']]\n",
    "xx=xx.fillna(0)\n",
    "\n",
    "y = df_new['label']\n",
    "x = xx\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "clf=RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_new = lda7\n",
    "df_new[['a','b','c','label','e']] = pd.get_dummies(lda5['dataset'])\n",
    "df_new['dataset']= label_encoder.fit_transform(df_new['dataset']) \n",
    "\n",
    "\n",
    "xx = df_new[['0','1','2','3','4','5','6','statuses_count','followers_count','friends_count','listed_count']]\n",
    "xx=xx.fillna(0)\n",
    "\n",
    "y = df_new['label']\n",
    "x = xx\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "clf=RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_new = lda8\n",
    "df_new[['a','b','c','label','e']] = pd.get_dummies(lda5['dataset'])\n",
    "df_new['dataset']= label_encoder.fit_transform(df_new['dataset']) \n",
    "\n",
    "\n",
    "xx = df_new[['0','1','2','3','4','5','6','7','statuses_count','followers_count','friends_count','listed_count']]\n",
    "xx=xx.fillna(0)\n",
    "\n",
    "y = df_new['label']\n",
    "x = xx\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "clf=RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "#____________________________________________________________________________________________________________________\n",
    "#SCRAP YARD\n",
    "df_master_tweet = pd.read_csv('/Users/willcarrington/Downloads/master_tweet.csv')\n",
    "df_master_user = pd.read_csv('/Users/willcarrington/Downloads/master_user.csv')\n",
    "\n",
    "i = 140000\n",
    "num = 140000*15\n",
    "\n",
    "df = df.sample(n=num)\n",
    "\n",
    "df_master_tweet = df.iloc[:140000]\n",
    "df_master_tweet1= df.iloc[140000:280000]\n",
    "df_master_tweet2= df.iloc[280000:420000]\n",
    "df_master_tweet3= df.iloc[420000:560000]\n",
    "df_master_tweet4= df.iloc[560000:700000]\n",
    "df_master_tweet5= df.iloc[700000:840000]\n",
    "df_master_tweet6= df.iloc[840000:980000]\n",
    "df_master_tweet7= df.iloc[980000:980000+i]\n",
    "df_master_tweet8= df.iloc[980000+i:980000+i+i]\n",
    "df_master_tweet9= df.iloc[980000+i+i:980000+i+i+i]\n",
    "df_master_tweet10= df.iloc[980000+i+i+i:980000+i+i+i+i]\n",
    "df_master_tweet11= df.iloc[980000+i+i+i+i:980000+i+i+i+i+i]\n",
    "df_master_tweet12= df.iloc[980000+i+i+i+i+i:980000+i+i+i+i+i+i]\n",
    "df_master_tweet13= df.iloc[980000+i+i+i+i+i+i:980000+i+i+i+i+i+i+i]\n",
    "df_master_tweet14= df.iloc[980000+i+i+i+i+i+i+i:980000+i+i+i+i+i+i+i+i]\n",
    "df_master_tweet15= df.iloc[980000+i+i+i+i+i+i+i+i:980000+i+i+i+i+i+i+i+i+i]\n",
    "\n",
    "df_all = pd.merge(df_master_tweet,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet1,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all1 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet2,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all2 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet3,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all3 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet4,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all4 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet5,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all5 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet6,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all6 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet7,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all7 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet8,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all8 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet9,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all9 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet10,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all10 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet11,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all11 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet12,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all12 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet13,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all13 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet14,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all14 = df_all.drop_duplicates()\n",
    "\n",
    "df_all = pd.merge(df_master_tweet15,df_master_user,on='check')\n",
    "df_all = df_all[df_all['lang']=='en']\n",
    "df_all = df_all[['text','check','dataset','statuses_count','followers_count','friends_count','listed_count']]\n",
    "df_all['text'] = df_all.groupby(['check'])['text'].transform(lambda x : ''.join(str(x)))\n",
    "df_all15 = df_all.drop_duplicates()\n",
    "\n",
    "pieces = (df_all1,df_all2,df_all3,df_all4,df_all5,df_all6,df_all7,df_all8,df_all9,df_all10,df_all11,df_all12,df_all13,df_all14,df_all15)\n",
    "df_master_split = pd.concat(pieces, ignore_index = True)\n",
    "df_master_split.head(500000)\n",
    "\n",
    "print(len(df_master_split[df_master_split['dataset']=='TFP']))\n",
    "print(len(df_master_split[df_master_split['dataset']=='FSF']))\n",
    "print(len(df_master_split[df_master_split['dataset']=='E13']))\n",
    "print(len(df_master_split[df_master_split['dataset']=='INT']))\n",
    "print(len(df_master_split[df_master_split['dataset']=='TWT']))\n",
    "\n",
    "number_real =1138\n",
    "number_fake = 284\n",
    "\n",
    "tfp_df = df_master_split[df_master_split['dataset']=='TFP'].sample(n=number_real)\n",
    "int_df = df_master_split[df_master_split['dataset']=='INT'].sample(n=number_fake)\n",
    "e13_df = df_master_split[df_master_split['dataset']=='E13'].sample(n=number_fake)\n",
    "fsf_df = df_master_split[df_master_split['dataset']=='FSF'].sample(n=number_fake)\n",
    "twt_df = df_master_split[df_master_split['dataset']=='TWT'].sample(n=number_fake)\n",
    "pcs = (tfp_df,int_df,e13_df,fsf_df,twt_df)\n",
    "df_master_split = pd.concat(pcs, ignore_index = True)\n",
    "\n",
    "min_chars = 4\n",
    "max_feats = 70000\n",
    "min_freq = 15\n",
    "\n",
    "\n",
    "df_master_split['tweet_re'] = df_master_split.text.str.replace('[^a-zA-Z]', ' ')\n",
    "df_master_split['tweet_re'] = df_master_split['text'].map(lambda x:x.lower())\n",
    "\n",
    "df_master_split = df_master_split.replace({'object':' '}, regex = True)\n",
    "df_master_split = df_master_split.replace({'int':' '}, regex = True)\n",
    "df_master_split = df_master_split.replace({'http':' '}, regex = True)\n",
    "df_master_split = df_master_split.replace({'dtype':' '}, regex = True)\n",
    "\n",
    "\n",
    "df_master_split['tweet_no_stop'] = df_master_split['tweet_re'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))\n",
    "\n",
    "df_master_split['tweet_token'] = df_master_split['tweet_no_stop'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data_lemmatized = lemmatization(df_master_split['tweet_token'], allowed_postags=['NOUN', 'VERB'])\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=min_freq,\n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  \n",
    "                             max_features=max_feats)\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "\n",
    "xxx=[2,3,4,5,6,7,8,9]\n",
    "chart = {'perp':[],'score':[]}\n",
    "for i in xxx:\n",
    "    lda_model = LatentDirichletAllocation(n_components=i,         \n",
    "                      max_iter=20,               \n",
    "                      learning_method='online',   \n",
    "                      random_state=100,          \n",
    "                      batch_size=128,            \n",
    "                      evaluate_every = -1,       \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "    chart['score'].append(lda_model.score(data_vectorized))\n",
    "    chart['perp'].append(lda_model.perplexity(data_vectorized))\n",
    "chart = pd.DataFrame(chart)\n",
    "\n",
    "start_time = time.time()\n",
    "importances = clf.feature_importances_\n",
    "predictions = clf.predict_proba(X_train)\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.9f} seconds\")\n",
    "Out\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "forest_importances = pd.Series(importances,index = range(8))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
